#!/usr/bin/env ruby
# frozen_string_literal: true

# ollama-client CLI — strict, JSON-first interface
# Uses the same Ollama::Client contracts as the library.
# No interactive chat. No hidden behavior.

require_relative "../lib/ollama_client"
require "optparse"

module OllamaCLI
  COMMANDS = %w[generate embed models pull].freeze

  # Parse top-level command
  def self.run(args = ARGV)
    if args.empty? || %w[-h --help help].include?(args.first)
      print_usage
      exit 0
    end

    command = args.shift

    unless COMMANDS.include?(command)
      warn "Unknown command: #{command}"
      warn "Available: #{COMMANDS.join(", ")}"
      exit 1
    end

    send(:"cmd_#{command}", args)
  rescue Ollama::Error => e
    output_error(e)
    exit 1
  end

  # ── generate ──────────────────────────────────────

  def self.cmd_generate(args)
    options = { json: false }
    parser = OptionParser.new do |opts|
      opts.banner = "Usage: ollama-client generate [options]"
      opts.on("--prompt PROMPT", "Prompt text (required)") { |v| options[:prompt] = v }
      opts.on("--model MODEL", "Model name") { |v| options[:model] = v }
      opts.on("--schema FILE", "JSON schema file for structured output") { |v| options[:schema] = v }
      opts.on("--json", "Force JSON output wrapper") { options[:json] = true }
      opts.on("--stream", "Stream tokens to stdout") { options[:stream] = true }
      opts.on("-h", "--help", "Show this help") do
        puts opts
        exit 0
      end
    end
    parser.parse!(args)

    unless options[:prompt]
      warn "Error: --prompt is required"
      warn parser
      exit 1
    end

    schema = nil
    schema = JSON.parse(File.read(options[:schema])) if options[:schema]

    client = build_client(options[:model])

    hooks = {}
    if options[:stream]
      hooks[:on_token] = lambda { |token|
        print token
        $stdout.flush
      }
      hooks[:on_complete] = -> { puts }
    end

    result = client.generate(prompt: options[:prompt], schema: schema, hooks: hooks)

    if options[:json] || schema
      puts JSON.pretty_generate(
        { status: "ok", model: client.instance_variable_get(:@config).model, result: result }
      )
    else
      puts result unless options[:stream] # already printed via hooks
    end
  end

  # ── embed ─────────────────────────────────────────

  def self.cmd_embed(args)
    options = {}
    parser = OptionParser.new do |opts|
      opts.banner = "Usage: ollama-client embed [options]"
      opts.on("--input TEXT", "Text to embed (required)") { |v| options[:input] = v }
      opts.on("--model MODEL", "Embedding model") { |v| options[:model] = v }
      opts.on("-h", "--help", "Show this help") do
        puts opts
        exit 0
      end
    end
    parser.parse!(args)

    unless options[:input]
      warn "Error: --input is required"
      warn parser
      exit 1
    end

    client = build_client
    model = options[:model] || "nomic-embed-text"
    vectors = client.embeddings.embed(model: model, input: options[:input])

    puts JSON.pretty_generate({ status: "ok", model: model, dimensions: vectors.length, embedding: vectors })
  end

  # ── models ────────────────────────────────────────

  def self.cmd_models(args)
    OptionParser.new do |opts|
      opts.banner = "Usage: ollama-client models"
      opts.on("-h", "--help", "Show this help") do
        puts opts
        exit 0
      end
    end.parse!(args)

    client = build_client
    models = client.list_models

    puts JSON.pretty_generate({ status: "ok", count: models.length, models: models })
  end

  # ── pull ──────────────────────────────────────────

  def self.cmd_pull(args)
    if args.empty? || args.first.start_with?("-")
      warn "Usage: ollama-client pull MODEL_NAME"
      exit 1
    end

    model_name = args.shift
    client = build_client
    client.pull(model_name)

    puts JSON.pretty_generate({ status: "ok", pulled: model_name })
  end

  # ── helpers ───────────────────────────────────────

  def self.build_client(model = nil)
    config = Ollama::Config.new
    config.base_url = ENV["OLLAMA_BASE_URL"] if ENV["OLLAMA_BASE_URL"]
    config.model = model if model
    Ollama::Client.new(config: config)
  end

  def self.output_error(error)
    payload = { status: "error", error: error.class.name, message: error.message }
    warn JSON.pretty_generate(payload)
  end

  def self.print_usage
    puts <<~USAGE
      ollama-client v#{Ollama::VERSION} — Production-safe Ollama CLI

      Usage: ollama-client <command> [options]

      Commands:
        generate   Generate text or structured JSON from a prompt
        embed      Generate embedding vectors for text
        models     List locally available models
        pull       Pull/download a model from Ollama registry

      Options:
        -h, --help Show help for any command

      Examples:
        ollama-client generate --prompt "Explain Ruby blocks" --json
        ollama-client generate --prompt "Classify this" --schema schema.json
        ollama-client generate --prompt "Write a poem" --stream
        ollama-client embed --input "What is Ruby?" --model nomic-embed-text
        ollama-client models
        ollama-client pull llama3.2

      Environment:
        OLLAMA_BASE_URL  Override base URL (default: http://localhost:11434)
    USAGE
  end
end

OllamaCLI.run
